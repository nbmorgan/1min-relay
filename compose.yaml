services:
  memcached:
    image: memcached
    container_name: memcached
    restart: unless-stopped
    networks:
      - relay_network

  1min-relay:
    build: 
      context: .  # Assumes Dockerfile is in current directory
      dockerfile: Dockerfile
    container_name: 1min-relay-container
    restart: unless-stopped
    ports:
       - "127.0.0.1:5001:5001"
       #- "5001:5001"
    environment:
      # Specify a subset of 1min.ai models to expose (comma separated)
      #- SUBSET_OF_ONE_MIN_PERMITTED_MODELS="qwen3-max,qwen-plus,qwen-max,qwen-flash,claude-sonnet-4-5-20250929,claude-sonnet-4-20250514,claude-opus-4-5-20251101,claude-opus-4-20250514,claude-opus-4-1-20250805,claude-haiku-4-5-20251001,command-r-08-2024,deepseek-reasoner,deepseek-chat,gemini-3-pro-preview,gemini-2.5-pro,gemini-2.5-flash,magistral-small-latest,magistral-medium-latest,ministral-14b-latest,open-mistral-nemo,mistral-small-latest,mistral-medium-latest,mistral-large-latest,gpt-5.1-codex-mini,gpt-5.1-codex,o4-mini,o3-mini,gpt-5.2-pro,gpt-5.2,gpt-5.1,gpt-5-nano,gpt-5-mini,gpt-5-chat-latest,gpt-5,gpt-4o-mini,gpt-4o,gpt-4.1-nano,gpt-4.1-mini,gpt-4.1,gpt-4-turbo,gpt-3.5-turbo,o4-mini-deep-research,o3-pro,o3-deep-research,o3,sonar-reasoning-pro,sonar-reasoning,sonar-pro,sonar-deep-research,sonar,grok-4-fast-reasoning,grok-4-fast-non-reasoning,grok-4-0709,grok-3-mini,grok-3,meta/meta-llama-3.1-405b-instruct,meta/meta-llama-3-70b-instruct,meta/llama-4-scout-instruct,meta/llama-4-maverick-instruct,meta/llama-2-70b-chat,openai/gpt-oss-20b,openai/gpt-oss-120b"
      - SUBSET_OF_ONE_MIN_PERMITTED_MODELS=mistral-nemo,gpt-4o-mini,gpt-4o,deepseek-chat
      # Restrict model usage to the specified subset (True/False)
      - PERMIT_MODELS_FROM_SUBSET_ONLY=True
    depends_on:
      - memcached
    networks:
      - relay_network

networks:
  relay_network:
    driver: bridge

